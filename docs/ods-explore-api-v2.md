# Huwise's Explore API Reference Documentation

**Summary:** Introduction
       Status
       v2.1 Changelog
       Base URL
   Getting Started
       Step 1: Find dataset information
       Step 2: Build your ODSQL query
       Step 3: Build your request
    ...

*   Introduction
    *   Status
    *   v2.1 Changelog
    *   Base URL
*   Getting Started
    *   Step 1: Find dataset information
    *   Step 2: Build your ODSQL query
    *   Step 3: Build your request
    *   Next steps
*   Authentication
    *   Finding and generating API keys
    *   Providing API keys within requests
    *   Using OAuth2 authorization
*   Opendatasoft Query Language (ODSQL)
    *   Language elements
    *   Select clause
    *   Where clause
    *   Group by clause
    *   Order by clause
*   ODSQL functions
    *   length()
    *   now()
    *   year()
    *   month()
    *   day()
    *   hour()
    *   minute()
    *   second()
    *   date\_format()
    *   json\_format()
    *   ifnull()
    *   lower()
    *   include() and exclude()
    *   Arithmetic operators
    *   random()
    *   distance()
    *   vector\_similarity()
*   ODSQL predicates
    *   search()
    *   suggest()
    *   vector\_similarity\_threshold()
    *   startswith()
    *   within\_distance()
    *   in\_bbox()
    *   intersects()
    *   disjoint()
    *   within()
    *   Text comparison operators
    *   Numeric comparison operators
    *   Date comparison operators
    *   Boolean field filter
    *   IN filter
    *   IS NULL filter
*   ODSQL aggregate functions
    *   avg()
    *   count()
    *   count(distinct)
    *   envelope()
    *   bbox()
    *   max()
    *   median()
    *   min()
    *   percentile()
    *   sum()
*   ODSQL grouping functions
    *   range() - group by static ranges
    *   range() - group by ranges of equal widths
    *   geo\_cluster()
*   Versioning
    *   Deprecation warnings
*   Catalog
    *   getQuery catalog datasets
    *   getList export formats
    *   getExport a catalog
    *   getExport a catalog in CSV
    *   getExport a catalog in RDF/XML (DCAT)
    *   getList facet values
    *   getShow dataset information
*   Dataset
    *   getQuery dataset records
    *   getList export formats
    *   getExport a dataset
    *   getExport a dataset in CSV
    *   getExport a dataset in Parquet
    *   getExport a dataset in GPX
    *   getList dataset facets
    *   getList dataset attachments
    *   getRead a dataset record

[API docs by Redocly](https://redocly.com/redoc/)

# Huwise's Explore API Reference Documentation (v2.1)

Download OpenAPI specification:Download

E-mail: [support@huwise.com](mailto:support@huwise.com) License: [Copyright Huwise](https://legal.huwise.com/en/terms-of-use.html)

# Introduction

Welcome to the Huwise Explore API!

The Huwise Explore API v2 is organized around REST. It provides access to all the data available through the platform in a coherent, hierarchical way.

If you want to learn more about how to make the most out of the API, we recommend reading the article [Introduction to API explore](https://userguide.huwise.com/l/fr/article/b3dyxp1acz-une-introduction-l-api-explore). This article will provide you with a comprehensive overview of the API's capabilities and guide you through the process of accessing and manipulating data.

*   Only the HTTP `GET` method is supported.
*   All API endpoints return JSON.
*   Endpoints are organized in a hierarchical way describing the relative relationship between objects.
*   All responses contain a list of links allowing easy and relevant navigation through the API endpoints.
*   All endpoints use the Opendatasoft Query Language (ODSQL). This means that most of the time, parameters work the same way for all endpoints.
*   While the `records` endpoint is subject to a [limited number of returned records](https://help.huwise.com/apis/ods-explore-v2/#tag/Dataset/operation/getRecords), the `exports` endpoint has no limitations.

## Status

The v2.1 version is stable and production ready: no breaking change will be introduced in the future, following our versioning policy.

## v2.1 Changelog

Following our versioning policy, some changes have been introduced in the 2.1 version of the Explore API. Some of them may not be backward compatible with the previous version:

*   the `group_by` clause is now available on export endpoints
*   the XLSX export format replaces the old XLS format
*   full text searches have been reworked:
    *   the search() function has been changed,
    *   two new functions are available: suggest() and startswith()
*   date functions (year(), month(), day(), hour(), minute(), second()) now return integers, instead of strings
*   grouping by geo fields is now deprecated and the `geo_cluster()` grouping function should be used instead
*   in a `group_by`, null values are now grouped
*   date aggregation keys are formatted as isoformat dates, previously as timestamps
*   CSV exports output a Byte Marker Order (BOM) character by default
*   GPX exports use `<extension>` instead of `<desc>` by default to export attributes
*   datetimes in GeojSON exports are now output as isoformat strings, they were previously output as integer timestamps
*   geometry functions have been renamed (old function names are still available):
    *   `distance()` becomes `within_distance()`
    *   `geometry(geom1, geom2, INTERSECTS)` becomes `intersects()`
    *   `geometry(geom1, geom2, DISJOINT)` becomes `disjoint()`
    *   `geometry(geom1, geom2, WITHIN)` becomes `within()`

Regarding endpoints:

*   `/api/explore/v2.0/<source>/datasets/<dataset_id>/aggregates` and `/api/explore/v2.0/<source>/datasets/<dataset_id>/query` are not available anymore in v2.1. Their functionalities have been merged into the unique "records" endpoint (`/api/explore/v2.0/<source>/datasets/<dataset_id>/records`),
*   The output schemas of the following endpoints have been changed:
    *   `/api/explore/v2.1/<source>/datasets`
    *   `/api/explore/v2.1/<source>/datasets/<dataset_id>`
    *   `/api/explore/v2.1/<source>/datasets/<dataset_id>/records`
    *   `/api/explore/v2.1/<source>/datasets/<dataset_id>/records/<record_id>`

## Base URL

The Explore API is accessed using a base URL that is specific to a domain. In the examples provided in the documentation, we use the domain [http://documentation-resources.huwise.com](http://documentation-resources.huwise.com).

URL paths start with `/api/explore/v2.1`. A path to a resource looks like this: `https://<domain>/api/explore/v2.1/<resource>`.

# Getting Started

To try out the Explore API, you'll get the first two records from a dataset that includes monthly prices for gold.

You'll get data from a portal providing public access to data. So, you don't need an account or an API key.

## Step 1: Find dataset information

For this example, you will use the GET records endpoint to list records from a dataset. So, you need the dataset identifier and the name of the field that stores the dates to build your request.

1.  Open a browser and go to the desired domain: `https://documentation-resources.huwise.com/explore`.

    The `explore` page lists all datasets on the domain.

2.  Click the desired dataset.


For this example, click "Gold Prices".

3.  Open the **Information** tab and check the **dataset identifier**.

In this example, the dataset identifier is `gold-prices`.

## Step 2: Build your ODSQL query

In this example, you just want to retrieve the first set of two dataset records. The records returned by the request are grouped into pages. Pages are limited in size according to the number of records per page as specified by the `limit` parameter.

1.  To determine how many records to retrieve, start the query with a `limit` parameter. Since you only want to retrieve the first two records, use `2` as the parameter value.
2.  To determine the specific page of data to be returned, use an `offset` parameter. Since you only want to retrieve the first set of two records, use `0` as the parameter value or keep it unspecified, since `0` is the default value.
3.  Put all the elements together.

The complete query is `limit=2&offset=0`.

## Step 3: Build your request

To retrieve data using the Explore API, use the `GET` HTTP method.

The path to a resource is made up of the following elements:

*   A domain: in this example, query the Explore API on the `documentation-resources.huwise.com` domain.
*   A resource: from the domain's `catalog`, retrieve the `records` of the `gold-prices` dataset.
*   A query: use the `select=count(*)&group_by=year(date)` query built in the previous step.

The complete path is `https://documentation-resources.huwise.com/api/explore/v2.1/catalog/datasets/gold-prices/records?limit=2`

You can access the Explore API using curl or any HTTP client.

To make a query using curl, open a terminal and paste the following command:

```shell
curl -X GET "https://documentation-resources.huwise.com/api/explore/v2.1/catalog/datasets/gold-prices/records?limit=2"
```

The response should look like this:

```json
{
  "total_count": 384,
  "results": [
    {
      "date": "1988-01",
      "price": 477.758
    },
    {
      "date": "1988-03",
      "price": 443.491
    }
  ]
}
```

`total_count` shows the total number of records returned by the query.

The `results` array contains the two records returned by the request. In case you use a `group_by`, the `total_count` shows the **available** total number of groups returned by the API (with a maximum of 20000).

For example, we can see the first record contains two fields: `date` and `price`. Given the information from the first record, in January 1988 (`1988-01`), the gold price was $477.758 (`477.758`).

## Next steps

Read the Opendatasoft Query Language (ODSQL) reference documentation. It will help you build queries, search and filter data from Huwise portals.

# Authentication

An authenticated user can be granted access to restricted datasets and benefit from extended quotas for API calls. The API features an authentication mechanism for users to be granted their specific authorizations.

For the platform to authenticate a user, you need to either:

*   be logged in a portal, so a session cookie authenticating your user is passed along your API calls, or
*   provide an API key via the Authorization header or as a query parameter.

## Finding and generating API keys

API keys are managed via your user profile page at `https://<domain>/account/` or by clicking on your name in the header.

Go to the tab named My API keys to see your existing API keys, revoke them and create new ones.

**Note:** By default, every API key authenticates requests as coming from your user, which means they grant the same rights (yours) to any person using them. Therefore, you should not share your keys. For advanced usages, API key permissions can be edited using the API key Automation API.

## Providing API keys within requests

If you try to access a private portal's catalog without being authenticated, the API returns a `401 Unauthorized` error.

After generating an API key, you can use it to make authenticated requests. Depending on the permissions granted to the user for which the API key has been created, the JSON response contains only data about the datasets this user can access on the portal.

It is good practice to pass the API key to the `Authorization` header in the following format:

`Authorization: Apikey <API_KEY>`

Alternatively, you can pass the API key as a query parameter in the following format:

`apikey=<API_KEY>`

Replace `<API_KEY>`with your API key.

**Note:** We recommend passing the API key via a header over in a query parameter because headers are not stored in your browser history or server logs, minimizing the risk of exposure of your API key.

## Using OAuth2 authorization

Huwise implements the OAuth2 authorization flow, allowing third-party application makers to access the data hosted on an Huwise platform on behalf of a user while never having to deal with a password, avoiding any user credential being compromised.

The Huwise OAuth2 authorization flow is compliant with RFC 6749 and uses Bearer Tokens in compliance with RFC 6750.

Application developers who want to use the Huwise APIs with OAuth2 must go through the following steps:

1.  Register their application with the Huwise platform.
2.  Request approval from users via an OAuth2 authorization grant.
3.  Request a bearer token that will allow them to query the Huwise platform APIs for a limited amount of time.
4.  Refresh the Bearer Token when it expires.

Currently, applications are registered on a specific domain and can only access data on this domain.

### Register an application for OAuth2 authentication

1.  Go to the My applications tab of your account page on the domain you want to register the application on.
2.  Fill the registration form with the following information:
    *   Application name: the name of the application
    *   Type:
        *   confidential: client password is kept secret from the user and only used from a trusted environment (e.g., a web service, where the client password is stored server-side and never sent to the user)
        *   public: client password is embedded in a client-side application, making it potentially available to the world (e.g., a mobile or desktop application)
    *   Redirection URL: the URL users will be redirected to after they have granted you permission to access their data
3.  Store the resulting client ID and client secret that will be needed to perform the next steps.

### Getting an authorization grant

1.  Redirect users to /oauth2/authorize/ with the appropriate query parameters.
2.  The user will then be authenticated in the platform and redirected to a page identifying your application.
3.  From there, the user will review the information you filled in the form described above and the scope of the requested access and grant your application the right to access their data.
4.  Once the user has accepted those terms, they will be redirected to your application's redirection URL with query parameters describing your authorization grant.

The query parameters you need to supply when redirecting the user are the following:

*   `client_id`: the client ID you were given during registration
*   `redirect_uri`: the redirect URI you provided during registration
*   `response_type`: this should always be set to code
*   `scopes` (optional): a list of space-separated requested scopes. Currently, only `all` is supported.
*   `state` (optional): a random string of your choice

The state parameter is not mandatory, but providing one is recommended for security reasons to verify the returned value provided in the authorization grant redirect.

> Example of a call to `/oauth2/authorize/`:

```http
GET /oauth2/authorize/?
client_id=123456789&
redirect_uri=https://example.com&
response_type=code&
state=ilovedata&
scope=all HTTP/1.1
```

The authorization grant redirect will have these values:

*   `code`: a 30-characters-long authorization code
*   `state`: the state passed in the request described above

The 30-character authorization code must now be converted into a bearer token within 1 hour before expiring.

Here is an example of redirection following a successful authorization:

```http
HTTP/1.0 302 FOUND

Location:
https://example.com?state=ilovedata&code=gKnAQc2yIfdz2mY25xxgpTY2uyG5Sv
```

### Converting an authorization grant to a bearer token

To receive a bearer token, convert the previously obtained authorization grant via a POST request to `/oauth2/token/` with the following parameters:

*   `client_id`: the client ID you were given during registration
*   `client_secret`: the client secret you were given during registration
*   `redirect_uri`: the redirect URI you provided during registration
*   `grant_type`: this should always be set to `authorization_code`
*   `code`: the 30-character authorization code received as an authorization grant
*   `scopes` *(optional)*: a list of space-separated requested scopes. Currently, only `all` is supported.
*   `state` *(optional)*: a random string of your choice

Example call to `/oauth2/token/`:

```http
POST /oauth2/token/ HTTP/1.1

client_id=cid&
client_secret=csc&
grant_type=authorization_code&
code=GokshWxRFXmW0MaLHkDv5HrG6wieGs&
scopes=all&
redirect_uri=https://example.com&
state=ilovedata
```

Alternative call with an `Authorization` header:

```http
POST /oauth2/token/ HTTP/1.1

Authorization: Basic Y2lkOmNzYw==

grant_type=authorization_code&
code=GokshWxRFXmW0MaLHkDv5HrG6wieGs&
scopes=all&
redirect_uri=https://example.com&state=ilovedata
```

Alternatively, you can pass your client ID and client secret through the Authorization header

The response to this request is a JSON representation of a bearer token, which contains the following values:

*   `access_token`: the token you can use to access the user's data.
*   `expires_in`: the number of seconds before token expiration
*   `token_type`: the type of the token. It will always be `Bearer`
*   `state`: the state passed in the request described above
*   `scope`: the list of scopes of this authorization code
*   `refresh_token`: a refresh token that can be used to renew this bearer token when expired

> Unlike the access token, which can be used any number of times until expiration, the refresh token doesn't expire but can only be used once.

Example response for a bearer token request:

```http
HTTP/1.0 200 OK

Content-Type: application/json
```

```json
{
  "access_token": "9kxoTUYvSxnAiMpv008NBqRiqk5xWt",
  "expires_in": 3600,
  "token_type": "Bearer",
  "state": "ilovedata",
  "scope": "all",
  "refresh_token": "jFfDUcsK9zzNMs1zwczzJxGrimPtmf"
}
```

### Using the bearer token

The bearer token can be passed along requests for authentication in three different ways:

*   as a query parameter of the request

```http
GET /api/end/point?access_token=9kxoTUYvSxnAiMpv008NBqRiqk5xWt HTTP/1.1
```

*   in the request's `Authorization` header

```http
GET /api/end/point HTTP/1.1

Authorization: Bearer 9kxoTUYvSxnAiMpv008NBqRiqk5xWt
```

*   in the request body

```http
GET /api/end/point HTTP/1.1

access_token=9kxoTUYvSxnAiMpv008NBqRiqk5xWt
```

### Refreshing a bearer token

To refresh an expired bearer token, send a request to the `/oauth2/token/` endpoint, with the following query parameters:

*   `client_id`: the client ID you were given during registration
*   `client_secret`: the client secret you were given during registration
*   `refresh_token`: the refresh token returned in the bearer token response
*   `grant_type`: this should always be set to `refresh_token`
*   `scopes`: a list of space-separated requested scopes. Currently, only `all` is supported.
*   `state` *(optional)*: a random string of your choice

The response to this request is identical to the bearer token response.

Example token refresh call:

```http
POST /oauth2/token/ HTTP/1.1

client_id=cid&
client_secret=csc&
grant_type=refresh_token&
refresh_token=jFfDUcsK9zzNMs1zwczzJxGrimPtmf&
scopes=all&
redirect_uri=https://example.com&
state=ilovedata
```

# Opendatasoft Query Language (ODSQL)

Filtering features are built in the core of the Huwise API engine.

The Opendatasoft Query Language (ODSQL) makes it possible to express complex queries as a filtering context for datasets or records and build aggregations or computed fields.

A given filtering context can simply be copied from one API to another. For example, it is possible to build a user interface that allows the user to visually select the records they are interested in, using full-text search, facets, and geo-filtering. Then, it allows them to download these records with the same filtering context.

The ODSQL is split into five different kinds of clauses:

*   The `select` clause allows choosing the returned fields, giving them an alias, manipulating them with functions like count, sum, min, max, etc.
*   The `where` clause acts as a filter for the returned datasets or records, thanks to boolean operations, filter functions, arithmetic expressions, etc.
*   The `group by` clause allows aggregating rows together based on fields, numeric ranges, or dates.
*   The `order by` and `limit` clauses allow choosing the order and quantity of rows received as a response.

These clauses are used as parameters in the Explore API v2 for searching, aggregating, and exporting datasets and records. Depending on the used endpoint, some features of the query language are available or not in the request.

**Note:** the whole query language is case insensitive, and spaces are optional. In this documentation, the uppercase is used for language keywords, only for clarity purposes.

## Language elements

ODSQL clauses are composed of basic language elements. These can either be field names or aliases, literals or reserved keywords.

### Field names

A field name is made of alphanumeric characters and underscores and refers to a field of a dataset or to a dynamically created field that only exists during the query (a.k.a. an alias).

**Note:** if a field name contains only numbers or is a keyword, it must be enclosed in back quotes.

> Examples of a field names:

```sql
my_field > 10  -- my_field is a field name

`12` > 10  -- without back quotes, 12 would be considered a numeric literal

`and`: "value" -- AND is a keyword, `and` represents a field name then
```

### Literals in ODSQL clauses

Literals are fixed values of a specific type and can be used in comparison, assignments, or functions.

There are 6 types of literal:

*   string
*   numeric
*   date
*   boolean
*   geometry
*   null

#### String literal

A string literal is a literal enclosed in either single or double quotes.

> Examples of a string literal:

```sql
"Word"

"Multiple words"

'Using single quotes'
```

**Note:** `\` (backslash) character can be used to escape special characters. For example to escape a single quote: `'Don\'t worry'`.

#### Numeric literal

A numeric literal is either an integer or a decimal value. It is not enclosed in quotes.

> Examples of numeric literals:

```sql
100

5.8

my_field > 108.7
```

#### Date literal

A date literal is defined with a `date` keyword followed by a valid date format enclosed in single quotes.

A valid date can be:

*   an [ISO 8601 date](https://en.wikipedia.org/wiki/ISO_8601), or
*   a slash-separated date in the YYYY/MM/DD (year/month/day) format.

> Examples of a date literal:

```sql
date'2017-04-03T08:02'

date'2018/04/01'
```

#### Boolean literal

A boolean literal can either be a `TRUE` or a `FALSE` keyword (case insensitive). It should be used in boolean filters.

> Example of a boolean literal:

```sql
my_boolean_field is TRUE

my_boolean_field: FALSE
```

#### Geometry literal

A geometry literal is defined with a `geom` keyword followed by a valid geometry expression enclosed in single quotes.

Supported geometry expressions are:

*   [WKT/WKB](https://en.wikipedia.org/wiki/Well-known_text)
*   [GeoJSON geometry](https://en.wikipedia.org/wiki/GeoJSON)

> Example of a geometry literal:

```sql
within_distance(my_geo_field, geom'POINT(1 1)', 10km)

geometry(my_geo_field, geom'{"type": "Polygon","coordinates":[[[100.0,
0.0],[101.0, 0.0],[101.0, 1.0],[100.0, 1.0],[100.0,0.0]]]}')
```

#### Null literal

The `null` literal (case insensitive) is used to represent the absence of a value.

It is present in the is null filter to test whether a field has a value or not.

### Reserved keywords in ODSQL clauses

Reserved keywords can be used inside clauses for building ODSQL expressions.

When used in a clause as a field literal, the reserved keyword must be escaped with back quotes.

List of reserved keywords:

*   and
*   as
*   asc
*   avg
*   by
*   count
*   date\_format
*   day
*   dayofweek
*   desc
*   distinct
*   equi
*   false
*   group
*   hour
*   ifnull
*   or
*   limit
*   lower
*   max
*   millisecond
*   min
*   minute
*   month
*   not
*   null
*   quarter
*   range
*   search
*   second
*   select
*   sum
*   top
*   true
*   upper
*   where
*   year

For example, `not` is a reserved keyword and must be escaped with back quotes if referred to as a field literal:

```sql
my_field_literal is not true -- my_field_literal is not a reserved keyword, there's no need to escape it

`not` is not true -- not is a reserved keyword and must be escaped
```

### Handling null values

A `null` value in a dataset is used when the value in a field is unknown or missing. It means that there is no data for a field in a record.

Each clause behaves differently to handle null values:

*   When selecting a field in a `select` clause, null values are represented as `null`.
*   When filtering with a `where` clause, a comparison involving at least one null value is false, meaning that null values are filtered out of the result.
*   When grouping with a `group_by` clause, no group exists for null values in v2.0, a null group do exist starting with v2.1
*   When sorting with an `order_by` clause, null values come after all other values, regardless of the sorting direction (i.e., ascending or descending).

Default handling of null values can be changed by filtering using the is null filter or replacing null values by an alternative value or expression using the `ifnull` function.

## Select clause

The select clause can be used in records Explore APIs as the parameter `select`.

The select clause allows:

*   choosing the fields that will be returned for each row,
*   transforming fields using arithmetic,
*   renaming fields,
*   adding computed virtual fields to fields, and
*   including or excluding fields based on a pattern.

A select clause is composed of a single select expression or a list of comma-separated expressions.

A select expression can be:

*   a field literal,
*   an include/exclude function,
*   an arithmetic expression, or
*   an aggregation function.

Except for the include/exclude function, a select expression can define a label with the keyword `AS`. This label will be used in the output of the API as `key` for the select expression result.

### Select field names

A select field name is the simplest form of select expression. It takes a field name that must be returned in the result. It also accepts the special character `*` to select all fields (it is the default behavior).

If a select expression is used in conjunction with a `group by` clause, the selected field name must be in the `group by` clause.

> Examples of a select field literal:

```sql
*                           -- Select all fields

field1, field2, field3      -- Only select field1, field2, and field3

field1 AS my_field, field2  -- Renaming field1 as my_field and select field2
```

### Select aggregation

Like in the SQL language, a `select` can also express an aggregation expression.

The following aggregation functions are available:

*   avg (average)
*   count
*   count distinct
*   envelope
*   max (maximum)
*   median
*   min (minimum)
*   percentile
*   sum

> Examples of an aggregation expression:

```sql
SUM(population) as sum_population -- Will compute the sum of all values for the field `population` returned as sum_population

COUNT(*) -- Return number of elements
```

## Where clause

The where clause can be used in the whole Explore API as the parameter `where`.

The where clause allows one to filter rows with a combination of boolean expressions.

A where expression can be:

*   a search query
*   a filter function
*   a comparison filter
*   an equality filter

Where expressions can be combined with boolean operators and grouped via parenthesis.

> Example of a where clause with boolean operators:

```sql
my_numeric_field > 10 and my_text_field like "paris" or within_distance(my_geo_field, geom'POINT(1 1)', 1km)
```

> This where clause filters results where numeric\_field > 10 and (my\_text\_field contains the word `paris` or distance between my\_geo\_field and the point with 1,1 as lat,lon is under 1 kilometer)

**Note**: it is generally possible to use multiple `where` clauses on an API endpoint. They are combined with a boolean `AND` in that case.

### Boolean operators

Where expressions can use boolean operators to express boolean filter.

There are 3 different boolean operations:

*   `AND`: results must match left and right expressions
*   `OR`: results must match left or right expression
*   `NOT`: inverses the next expression

`AND` has precedence over the `OR` operator. It means that, in the expression `a or b and c`, the sub-expression `b and c` is interpreted and executed first. It can also be written with parenthesis: `a or (b and c)`.

In order to change operator precedence, it is possible to use parenthesis in the expression. To give precedence to the `OR` operator, the above expression can be written `(a or b) and c`.

> Examples of a boolean operator:

```sql
my_boolean_field OR my_numeric_field > 50 and my_date_field > date'1972'
-- Results can have my_boolean_field to true. They can also have my_numeric_field greater than 50 and my_date_field older than 1972

(my_boolean_field OR my_numeric_field > 50) and my_date_field > date'1972'
-- Results must have my_date_field older than 1972. They also must have my_boolean_field to true or my_numeric_field greater than 50
```

### Search query filter

Filter search queries are queries that don't refer to fields. They only contain quoted strings and boolean operators. Filter search queries perform full-text searches on all visible fields of each record and return matching rows.

If the string contains more than one word, the query will be an `AND` query on each tokenized word.

> Examples of a search query:

```sql
"tree"

"tree" AND "flower"

"tree" OR "car"

NOT "dog"

"dog" AND NOT "cat"
```

> Examples of a search query with multiple words:

```sql
"film"           -- returns results that contain film

"action movies"  -- returns results that contain action and movies.
```

### Filter functions

Filter functions are built-in functions that can be used in a `where` clause.

Available filter functions are:

*   `search` function, to perform a full-text search
*   `suggest` function
*   `startswith` function
*   `in_bbox` function, to filter in a geographical area defined by a bounding box
*   `within_distance` function, to filter in a geographical area defined by a circle
*   `intersects`, `disjoint` and `within` to filter in a geographical area defined by a geometry

### Comparison operators

Three types of comparison operators can be used in a `where` clause:

*   text comparison operators
*   numeric comparison operators
*   date comparison operators

## Group by clause

The group by clause can be used in the Explore API as the parameter `group_by`.

The group by clause creates groups from data depending on a group by expression. Groups of data cannot be returned directly and aggregation functions in the `select` clause have to be used to "summarize" and return a value for each group. An operation of "aggregation" can then be described by two parts: the `group_by` part that make groups of rows of data from a specific criterion and an aggregation function in the `select` clause to reduce each group to a row.

A group by clause can contain:

*   a single group by expression, or
*   a list of comma-separated group by expressions.

Like select expressions, a group by expression can have an `AS` statement to give it a label.

A group by expression can be:

*   empty,
*   a field,
*   static ranges,
*   ranges of equal widths,
*   the result of a function applied on a field value (e.g. a date function, or a date format)

> Example of a simple group by expression with a label:

```sql
group_by=my_field as myfield
```

> Example of multiple group by expressions with a label:

```sql
group_by=my_field1,my_field2 as my_field
```

### Empty group by

When no `group_by` part is expressed, rows of data are implicitly grouped into an sole group and aggregation functions are computed on the whole set of records.

### Group by field

A group by field expression allows the grouping of specified field values. It creates a group for each different field value.

**Format:** `group_by=<field_literal>`

> Example of a simple group by field expression

```sql
group_by=my_field
```

**Note:**

*   Starting with v2.1, grouping by geopoint or geoshape fields is not supported directly anymore. Please use the geo\_cluster() grouping function to make groups out of geo points.
*   Starting with v2.1, grouping by a date field now formats the key of each group as a string representing the ISO formatting of the date, when it was an integer timestamp in v2.0

## Order by clause

The order by clause can be used to sort rows returned by a query.

The parameter `order_by` adds an order by clause to an API query. It accepts a list of comma-separated expressions followed by a direction:

*   ASC for ascending
*   DESC for descending

**Format:** `order_by = expression [ ASC | DESC ], ...`

An order by expression can be:

*   a field
*   an aggregation function
*   a `random` function

The direction, if not specified, is ASC (ascending) by default. The random sorting will circumvent the default direction.

**Note:** when ordering by both aggregations and fields, the aggregation order must be at the head of the list. For example, `order_by = avg(age), gender works`, but `order_by = gender, avg(age)` returns an error.

> Examples of an order by clause

```sql
group_by=city & order_by=city ASC -- Order cities alphabetically

group_by=city & order_by=count(*) DESC -- Order each city by its number of records

select=count(*) as population_count & group_by=city  & order_by=population_count DESC -- Order each city by its number of records, using a label

group_by=city, year(birth_date) as birth_year & order_by=city DESC, birth_year ASC -- Order by city and then by year of birth
```

# ODSQL functions

## length()

**Syntax:** `length(<text_literal>|<text_field>)`

**Returned type:** `integer`

**Clauses where it can be used:** `select`, `where`, `order_by`

Returns the string length of its parameter, i.e. the number of characters that composes the string.

## now()

**Syntax:** `now(<optional_named_parameters>)`

**Returned type:** `datetime`

**Clauses where it can be used:** `select`, `where`, `order_by`

### Parameters to the now() function

> Examples, assuming the current date time is 2021-05-06 12:34:55.450500+00:00, which is a Thursday

```sql
now() -- Returns '2021-05-06T12:34:55.450500+00:00'
now(year=2000) -- Sets the year component to return '2000-05-06T12:34:55.450500+00:00'
now(years=-1) -- Sets the year to one year ago which is '2020-05-06T12:34:55.450500+00:00'
now(year=2001, months=-1) -- Sets the year to 2001 and subtract 1 month to return '2000-04-06T12:34:55.450500+00:00'
now(day=31,month=2) -- Sets the day to 31, then the month to 2. The actual day part is rounded to 28 '2021-02-28T12:34:55.450500+00:00'
now(weekday=0) -- Sets the day to the next Monday which is '2021-05-10T12:34:55.450500+00:00'
now(mondays=+1) -- Sets the day to the next Monday which is also '2021-05-10T12:34:55.450500+00:00'
now(mondays=-1) -- Sets the day to the previous Monday which is '2021-05-03T12:34:55.450500+00:00'
```

Without any parameters, the `now()` function returns the current date and time.

The function may also be called with named parameters to set or modify certain parts of the current date and time.

With each parameter, an integer value is required, interpreted as an absolute value or as a relative value to a part of the current date and time.

Parameter names in their singular form will set a certain part of the current date and time to the given value. Parameter names written in plural will add or subtract the given value to a part of the current date and time.

If a parameter is used multiple times in the call, only the last one is actually used, the others are ignored.

Parameter name

Accepted values

Description

`year`

1 to 9999

Year component

`years`

Any integer

Value to add to or subtract from the year component

`month`

1 to 12

Month component

`months`

Any integer

Value to add to or subtract from the month component, then the year component in case of overflow

`day`

Any positive integer

Day component, rounded to the maximum valid day number for the current month

`days`

Any integer

Value to add to or subtract from the day component, then the month component in case of overflow

`hour`

0 to 23

Hour component

`hours`

Any integer

Value to add to or subtract from the hour component, then the day component in case of overflow

`minute`

0 to 59

Minute component

`minutes`

Any integer

Value to add to or subtract from the minute component, then the hour component in case of overflow

`second`

0 to 59

Second component

`seconds`

Any integer

Value to add to or subtract from the second component, then the minute component in case of overflow

`microsecond`

0 to 999999

Microsecond component

`microseconds`

Any integer

Value to add to or subtract from the microsecond component, then the second component in case of overflow

`weekday`

0 to 6

Day of the week, 0 for monday to 6 for sunday

`mondays`

Any integer

Number of Mondays to add to or subtract from the current date

`tuesdays`

Any integer

Number of Tuesdays to add to or subtract from the current date

`wednesdays`

Any integer

Number of Wednesdays to add to or subtract from the current date

`thursdays`

Any integer

Number of Thursdays to add to or subtract from the current date

`fridays`

Any integer

Number of Fridays to add to or subtract from the current date

`saturdays`

Any integer

Number of Saturdays to add to or subtract from the current date

`sundays`

Any integer

Number of Sundays to add to or subtract from the current date

## year()

**Syntax:** `year(<date_literal>|<date_field>|<datetime_literal>|<datetime_field>)`

**Returned type:** `integer`

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

Returns the year number of a date or datetime as a string.

## month()

**Syntax:** `month(<date_literal>|<date_field>|<datetime_literal>|<datetime_field>)`

**Returned type:** `integer`

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

Returns the month number (between 1 and 12) of a date or datetime as a string.

## day()

**Syntax:** `day(<date_literal>|<date_field>|<datetime_literal>|<datetime_field>)`

**Returned type:** `integer`

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

Returns the day number of the month (between 1 and 31) of a date or datetime as a string.

## hour()

**Syntax:** `hour(<date_literal>|<date_field>|<datetime_literal>|<datetime_field>)`

**Returned type:** `integer`

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

Returns the hour number (between 0 and 23) of a date or datetime as a string.

## minute()

**Syntax:** `minute(<date_literal>|<date_field>|<datetime_literal>|<datetime_field>)`

**Returned type:** `integer`

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

Returns the minute number (between 0 and 59) of a date or datetime as a string.

## second()

**Syntax:** `second(<date_literal>|<date_field>|<datetime_literal>|<datetime_field>)`

**Returned type:** `integer`

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

Returns the second number (between 0 and 59) of a date or datetime as a string.

## date\_format()

**Syntax:** `date_format(<date>, <date_format>)`

**Arguments:**

*   `<date>`: a date field,
*   `<date_format>`: a string describing how to format the date (see below)

**Returned type:** `string`.

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

`<date_format>` is a string, where each character or group of characters will be replaced by parts of the date in the returned string.

The following formats are available for a date format expression:

Symbol

Description

Examples

yy or YY

year on two digits

20

yyyy or YYYY

year on four digits

2020

xx

weekyear\* on two digits

96

xxxx

weekyear\* on four digits

1996

w

week of weekyear

7

ww

week of weekyear, left-padded with 0

07

e

day of week, as a number, 1 for Monday to 7 for Sunday

2

E

day of week, abbreviated name

sun.

EEEE

day of week, full name

Sunday

D

day of year

89

DDD

day of year, left-padded with 0

089

M

month of year

7

MM

month of year, left-padded with 0

07

MMMM

month of year, full name

July

d

day of month

8

dd

day of month, left-padded with 0

08

H

hour of day, 0-23

9

HH

hour of day, 00-23, left-padded with 0

09

m

minute of hour, 0-59

13

mm

minute of hour, 00-59, left-padded with 0

09

s

second of minute, 0-59

13

ss

second of minute, 00-59, left-padded with 0

09

\*Years and week years differ slightly. For more information, see the [definition](https://en.wikipedia.org/wiki/ISO_week_date) of week years.

The date format can contain free text that won't be interpreted. The free text must be surrounded by single quotes '.

To insert a single quote in the final string, it must be doubled.

Some special characters can also be used as delimiters between date components: `?`, `,`, `.`, `:`, `/` and `-`.

> Examples of a `date_format` function, where `date_field` = '2007-11-20T01:23:45':

```sql
date_format(date_field, 'dd/MM/YYYY') -- Returns '20/11/2007'

date_format(date_field, "'The date is 'dd/MM/YYYY") -- Returns 'The date is
20/11/2007'

date_format(date_field, "'The date is '''dd/MM/YYYY''") -- Returns "The date
is '20/11/2007'"

date_format(date_field, 'E') -- Returns 'mar.'

date_format(date_field, 'EEEE') -- Returns 'mardi'

date_format(date_field, 'H') -- Returns '1'

date_format(date_field, 'HH') -- Returns '01'

date_format(date_field, 'yy') -- Returns '07'

date_format(date_field, 'yyyy') -- Returns '2007'

date_format(date_field, 'M') -- Returns '11'

date_format(date_field, 'MM') -- Returns '11'
```

When used in the `where` clause, `date_format` must be compared to string values.

> Example of a `date_format` function used in a `where` clause:

```sql
where=date_format(date_field, 'dd') = '08'
```

You can use the `lang` parameter to force the output language.

## json\_format()

**Syntax:** `json_format(<text_field>,[<fallback>[<null>|<text_literal>]])`

**Returned type:** `text` or `json`

**Clause where it can be used:** `select`

**Description:**

Formats the text field into JSON if possible. If the text can be transformed into valid JSON, it returns the formatted JSON string. If the text cannot be transformed into valid JSON, it returns either the fallback value if provided or the original string.

*   `<text_field>` (mandatory): A text field to be formatted into JSON. It cannot be multivalued.

*   `<fallback>` (optional): A fallback string to return if the text cannot be transformed into valid JSON. If omitted, the original string is returned in case of invalid JSON.


**Note**: On `/exports`, except with the JSON export format, this function returns the original text value.

> Example of a `json_format` function used in a `select` clause:

```sql
select=json_format(text_field)

select=json_format(text_field, 'bad json')

select=json_format(text_field, null)
```

## ifnull()

**Syntax:** `ifnull(<expression>, <alternative_expression>)`

**Arguments:**

*   `<expression>`: a field or an expression
*   `<alternative_expression>`: an alternative field, expression or literal

**Clauses where it can be used:** `select`, `where`, `order_by`, `group_by`

**Returned type:** the type of `<expression>` when not null

**Returned value:** the result of `<alternative_expression>` if `<expression>` returns a null value. The result of `<expression>` otherwise.

The returned type of `<expression>` and `<alternative_expression>` should be the same.

For `group_by` clause, expressions are restricted to fields and literals.

> Examples of `ifnull` function, where `int_field` contains some null values:

```sql
ifnull(int_field, 0) -- value of int_field is 0 for each row that contains a null value
```

## lower()

**Syntax:** `lower(<text_literal>|<text_field>)`

**Returned type:** `string`

**Clauses where it can be used:** `select`, `where`, `order_by` and `group_by`

Returns a string in lowercase.

> Some examples:

```sql
lower('JAZZ') -- returns 'jazz'

lower(text_field) -- returns the lowercase representation of the field
```

## include() and exclude()

**Syntax:** `include(<field_name_pattern>)`

**Syntax:** `exclude(<field_name_pattern>)`

**Clauses where it can be used:** `select` only

Include and exclude are functions that accept fields names.

Fields listed in an include function are present in the result, whereas fields listed in an exclude function are absent from the result.

Fields can contain a wildcard suffix (the `*` character). In that case, the inclusion/exclusion works on all field names beginning with the value preceding the wildcard.

**Note**: `include()` and `exclude()` are pseudo functions: they do not return a value, but are used as a declaration to constrain the list of returned fields.

> Examples of an include/exclude:

```sql
include(pop) -- will only include fields which name is pop

exclude(pop) -- will exclude fields which name is pop

include(pop*) -- Will include fields beginning with pop
```

## Arithmetic operators

An arithmetic expression accepts simple arithmetic operations. It accepts field names, numeric constants or text values, and scalar functions. More complex arithmetic expressions can be formed by connecting these elements with arithmetic operators:

*   `+`: add
*   `-`: subtract
*   `*`: multiply
*   `/`: divide

**Note:** A division by zero returns a null value.

Arithmetic operators are only defined on numeric values.

> Examples of arithmetic expressions:

```sql
2 * population -- the value of the field `population` doubled

"hello" -- the constant string "hello"

length(country_name) -- the string length of the field `country_name`
```

## random()

**Syntax:** `random ( <integer> )`

**Clauses where it can be used:** `order_by` only

The `<integer>` is the seed of the random function. When using the random function with one same seed, it will return the same random order each time.

> Examples of an order by random

```sql
group_by=city & order_by=random(1) -- Order cities randomly

group_by=city & order_by=random(1) -- Order cities randomly in the exact same order as the first example

group_by=city & order_by=random(2) -- Order cities randomly in a different order than the first example
```

## distance()

**Syntax:** `distance(<geo_field>, <center_geometry>)`

**Clauses where it can be used:** `select`, `order_by`

**Returned type:** numeric

> Examples of a `distance` function:

```sql
distance(field_name, GEOM'<geometry>')
```

The `distance` function computes arc distance between geo\_point field and a point geometry as reference. Distance (in m) can be returned using `select` and/or used to sort records.

## vector\_similarity()

**Syntax:** `vector_similarity("<search query>")`

**Clauses where it can be used:** only `order_by`

**Returned type:** float

This function is able to compute a semantic distance between your search query and the catalog metadata, e.g the titles, keywords, themes and descriptions. It can only be used for a catalog search. The results will be semantically sorted, i.e. the first results will have some content where the meaning should be close to your search query. For instance if you search `"funny kitty"`, the results should have some content which contains the query terms but also `cats`, `fun`, `pets`, etc.

> Examples of a `vector_similarity` function:

```sql
vector_similarity("jazz concerts in nyc")
```

# ODSQL predicates

Predicates are functions that return a boolean value (`true` or `false`). They can be used to filter results in the `where` clause.

## search()

**Syntax:** `search(<text_field>|*, <text_literal>)` where:

*   first parameters are the set of fields on which the search is done:
    *   `*` or empty to search on all visible fields
    *   a subset of field names separated with a comma `,`
*   the string to search for as last parameter

**Clauses where it can be used:** `where` only

**Returned type:** boolean

The `search()` function performs a full-text query on all selected fields of each record and return matching records.

It is a fuzzy search and a prefix search: `<test_literal>` is first split into terms separated by a space, the first terms are searched for with a certain level of fuziness (see below), and the last term is a prefix search. The level of fuziness for each term depends on the length of the term:

*   for terms with a length > 5, it matches strings with a [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) of 2,
*   for terms with a length > 2, it matches strings with a [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) of 1

The matching is **case insensitive**.

> Examples of a `search` function:

```sql
search(title, "bok of secret")  -- will match "THE BOOK OF SECRETS"
```

## suggest()

**Syntax:** `suggest(<text_field>|*, <text_literal>)` where:

*   first parameters are the set of fields on which the search is done:
    *   `*` or empty to search on all visible fields
    *   a subset of field names separated with a comma `,`
*   the string to search for as last parameter

**Clauses where it can be used:** `where` only

**Returned type:** boolean

> Examples of a `suggest` function:

```sql
suggest(*, "film")  -- returns results that contain film, films, filmography, etc. in at least one visible field
suggest("film")  -- equivalent to the above query
search(title, "secret")  -- will match "THE BOOK OF SECRETS"

suggest(text_field, other_text_field, "film")  -- same search but in text_field or other_text_field
suggest(text_field, "film") OR suggest(other_text_field, "film") -- equivalent to the above query

suggest(text_field, "film") AND suggest(other_text_field, "film") -- returns results that contain film in both text_field and other_text_field
```

The `suggest()` function performs a partial word match on all selected fields of each record and returns matching records. Unlike `search()`, the matching must be exact (no fuzziness), but matches any part of a word.

The matching is **case insensitive**.

## vector\_similarity\_threshold()

**Syntax:** `vector_similarity_threshold("<search query>", <threshold>)`

**Clauses where it can be used:** only `where`

**Returned type:** boolean

This predicate performs a semantic vector search, returning only records where the semantic similarity exceeds a specified threshold. It operates similarly to the `vector_similarity()` function but filters results rather than ordering them.

The `threshold` parameter should be a float value between 0 and 1, where:

*   1.0 represents identical semantic meaning
*   0.0 represents no semantic relationship

The function compares the semantic meaning of your search query against catalog metadata including titles, keywords, themes, and descriptions.

> Examples of a `vector_similarity_threshold` function:

```sql
vector_similarity_threshold("jazz concerts in nyc", 0.7)  -- Returns only results with >70% semantic similarity

vector_similarity_threshold("climate change", 0.5) AND year > 2020  -- Combines semantic search with other filters
```

## startswith()

**Syntax:** `startswith(<text_field>, <text_literal>)`

**Clauses where it can be used:** `where` only

**Returned type:** boolean

The `startswith()` predicate checks if a field value starts with the specified text literal. The matching is **case insensitive**.

> Examples of a `startswith` function:

```sql
startswith(title, "THE")  -- matches "The Book", "THEATER", etc.

startswith(name, "john")  -- matches "John Smith", "Johnny", etc.
```

## within\_distance()

**Syntax:** `within_distance(<geo_field>, <center_geometry>, <distance>)`

**Clauses where it can be used:** `where` only

**Returned type:** boolean

The `within_distance()` predicate filters records based on geographic proximity. It returns records where the geo field is within the specified distance from a center point.

The distance parameter can use units:

*   `m` for meters
*   `km` for kilometers
*   `mi` for miles

> Examples of a `within_distance` function:

```sql
within_distance(location, geom'POINT(2.3522 48.8566)', 5km)  -- Within 5km of Paris center

within_distance(geo_field, geom'POINT(-74.006 40.7128)', 10mi)  -- Within 10 miles of NYC
```

## in\_bbox()

**Syntax:** `in_bbox(<geo_field>, <min_lat>, <min_lon>, <max_lat>, <max_lon>)`

**Clauses where it can be used:** `where` only

**Returned type:** boolean

The `in_bbox()` predicate filters records within a bounding box defined by minimum and maximum latitude and longitude coordinates.

> Examples of an `in_bbox` function:

```sql
in_bbox(location, 48.8, 2.3, 48.9, 2.4)  -- Records within a box in Paris

in_bbox(geo_field, -74.1, 40.7, -73.9, 40.8)  -- Records within a box in NYC
```

## intersects()

**Syntax:** `intersects(<geo_field>, <geometry>)`

**Clauses where it can be used:** `where` only

**Returned type:** boolean

The `intersects()` predicate returns true if the geo field intersects with the specified geometry.

> Examples of an `intersects` function:

```sql
intersects(shape, geom'POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))')

intersects(location, geom'{"type":"Polygon","coordinates":[[[2.3,48.8],[2.4,48.8],[2.4,48.9],[2.3,48.9],[2.3,48.8]]]}')
```

## disjoint()

**Syntax:** `disjoint(<geo_field>, <geometry>)`

**Clauses where it can be used:** `where` only

**Returned type:** boolean

The `disjoint()` predicate returns true if the geo field does not intersect with the specified geometry.

> Examples of a `disjoint` function:

```sql
disjoint(shape, geom'POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))')
```

## within()

**Syntax:** `within(<geo_field>, <geometry>)`

**Clauses where it can be used:** `where` only

**Returned type:** boolean

The `within()` predicate returns true if the geo field is completely within the specified geometry.

> Examples of a `within` function:

```sql
within(location, geom'POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))')
```

## Text comparison operators

Text comparison operators allow comparing text field values with text literals in the `where` clause.

### LIKE operator

**Syntax:** `<text_field> LIKE <text_literal>`

**Returned type:** boolean

The `LIKE` operator performs a case-insensitive substring match.

> Examples of LIKE operator:

```sql
title LIKE "book"  -- Matches if title contains "book" (case insensitive)

name LIKE "john"  -- Matches if name contains "john"
```

### Equality operator for text

**Syntax:** `<text_field> = <text_literal>`

**Returned type:** boolean

The equality operator performs an exact match (case sensitive by default).

> Examples of text equality:

```sql
status = "active"  -- Exact match for "active"

city = "Paris"  -- Exact match for "Paris"
```

## Numeric comparison operators

Numeric comparison operators can be used in the `where` clause to filter records based on numeric values.

Available operators:

*   `=`: equal to
*   `!=` or `<>`: not equal to
*   `>`: greater than
*   `>=`: greater than or equal to
*   `<`: less than
*   `<=`: less than or equal to

> Examples of numeric comparison:

```sql
population > 1000000  -- Cities with population greater than 1 million

price <= 100  -- Items with price less than or equal to 100

age >= 18 AND age < 65  -- Age between 18 and 64
```

## Date comparison operators

Date comparison operators can be used in the `where` clause to filter records based on date or datetime values.

Available operators:

*   `=`: equal to
*   `!=` or `<>`: not equal to
*   `>`: after
*   `>=`: on or after
*   `<`: before
*   `<=`: on or before

> Examples of date comparison:

```sql
date > date'2020-01-01'  -- Records after January 1, 2020

created_at <= date'2021/12/31'  -- Records created on or before December 31, 2021

updated >= date'2020-01-01' AND updated < date'2021-01-01'  -- Records updated in 2020
```

## Boolean field filter

Boolean fields can be filtered using the `IS` keyword with `TRUE` or `FALSE` literals.

**Syntax:** `<boolean_field> IS [TRUE|FALSE]`

**Returned type:** boolean

> Examples of boolean filters:

```sql
is_active IS TRUE  -- Records where is_active is true

published IS FALSE  -- Records where published is false
```

Alternatively, you can use simple field names:

```sql
is_active  -- Equivalent to: is_active IS TRUE

NOT is_active  -- Equivalent to: is_active IS FALSE
```

## IN filter

The `IN` operator allows filtering based on a list of values.

**Syntax:** `<field> IN (<value1>, <value2>, ...)`

**Returned type:** boolean

> Examples of IN filter:

```sql
status IN ("active", "pending", "approved")  -- Status is one of these values

country IN ("France", "Germany", "Spain")  -- Country is one of these

year IN (2019, 2020, 2021)  -- Year is one of these
```

## IS NULL filter

The `IS NULL` operator checks if a field has no value.

**Syntax:** `<field> IS [NOT] NULL`

**Returned type:** boolean

> Examples of IS NULL filter:

```sql
description IS NULL  -- Records where description has no value

email IS NOT NULL  -- Records where email has a value
```

# ODSQL aggregate functions

Aggregate functions are used in the `select` clause in combination with a `group_by` clause to compute statistics on groups of records.

## avg()

**Syntax:** `avg(<numeric_field>)`

**Returned type:** numeric

**Clauses where it can be used:** `select` only

Computes the average value of a numeric field across all records in each group.

> Examples of avg():

```sql
select=avg(price) & group_by=category  -- Average price per category

select=avg(age) AS average_age & group_by=city  -- Average age per city with alias
```

## count()

**Syntax:** `count(*)`

**Returned type:** integer

**Clauses where it can be used:** `select` only

Returns the number of records in each group.

> Examples of count():

```sql
select=count(*) & group_by=category  -- Number of records per category

select=count(*) AS total & group_by=year(date)  -- Number of records per year
```

## count(distinct)

**Syntax:** `count(distinct <field>)`

**Returned type:** integer

**Clauses where it can be used:** `select` only

Returns the number of distinct values for a field in each group.

> Examples of count(distinct):

```sql
select=count(distinct user_id) & group_by=city  -- Number of unique users per city

select=count(distinct product) AS unique_products & group_by=category
```

## envelope()

**Syntax:** `envelope(<geo_field>)`

**Returned type:** geometry (polygon)

**Clauses where it can be used:** `select` only

Returns the smallest rectangle (envelope) that contains all geometries in each group.

> Examples of envelope():

```sql
select=envelope(location) & group_by=region  -- Bounding envelope per region
```

## bbox()

**Syntax:** `bbox(<geo_field>)`

**Returned type:** array of coordinates

**Clauses where it can be used:** `select` only

Returns the bounding box coordinates `[min_lat, min_lon, max_lat, max_lon]` for all geometries in each group.

> Examples of bbox():

```sql
select=bbox(location) & group_by=city  -- Bounding box coordinates per city
```

## max()

**Syntax:** `max(<numeric_field>|<date_field>)`

**Returned type:** same as input field

**Clauses where it can be used:** `select` only

Returns the maximum value of a field across all records in each group.

> Examples of max():

```sql
select=max(price) & group_by=category  -- Highest price per category

select=max(date) AS latest_date & group_by=year(date)  -- Latest date per year
```

## median()

**Syntax:** `median(<numeric_field>)`

**Returned type:** numeric

**Clauses where it can be used:** `select` only

Returns the median value of a numeric field across all records in each group.

> Examples of median():

```sql
select=median(price) & group_by=category  -- Median price per category

select=median(age) AS median_age & group_by=city
```

## min()

**Syntax:** `min(<numeric_field>|<date_field>)`

**Returned type:** same as input field

**Clauses where it can be used:** `select` only

Returns the minimum value of a field across all records in each group.

> Examples of min():

```sql
select=min(price) & group_by=category  -- Lowest price per category

select=min(date) AS earliest_date & group_by=year(date)  -- Earliest date per year
```

## percentile()

**Syntax:** `percentile(<numeric_field>, <percentile_value>)`

**Arguments:**

*   `<numeric_field>`: the field to compute the percentile on
*   `<percentile_value>`: a number between 0 and 100

**Returned type:** numeric

**Clauses where it can be used:** `select` only

Returns the specified percentile value of a numeric field across all records in each group.

> Examples of percentile():

```sql
select=percentile(price, 95) & group_by=category  -- 95th percentile price per category

select=percentile(age, 50) AS median_age & group_by=city  -- 50th percentile (median)
```

## sum()

**Syntax:** `sum(<numeric_field>)`

**Returned type:** numeric

**Clauses where it can be used:** `select` only

Returns the sum of all values of a numeric field across all records in each group.

> Examples of sum():

```sql
select=sum(revenue) & group_by=category  -- Total revenue per category

select=sum(population) AS total_pop & group_by=region  -- Total population per region
```

# ODSQL grouping functions

Grouping functions are special functions that can be used in the `group_by` clause to create groups based on specific criteria.

## range() - group by static ranges

**Syntax:** `range(<numeric_field>, <range1_max>, <range2_max>, ...)`

**Clauses where it can be used:** `group_by` only

Groups records into static ranges based on a numeric field. Each parameter defines the upper bound of a range.

> Examples of static range grouping:

```sql
group_by=range(age, 18, 30, 50, 65)  -- Creates groups: <18, 18-30, 30-50, 50-65, 65+

select=count(*) & group_by=range(price, 100, 500, 1000)  -- Price ranges: <100, 100-500, 500-1000, 1000+
```

## range() - group by ranges of equal widths

**Syntax:** `range(<numeric_field>, <range_width>)`

**Clauses where it can be used:** `group_by` only

Groups records into ranges of equal width based on a numeric field.

> Examples of equal-width range grouping:

```sql
group_by=range(temperature, 10)  -- Groups by temperature ranges of 10 degrees

select=count(*) & group_by=range(price, 100)  -- Price ranges: 0-100, 100-200, 200-300, etc.
```

## geo\_cluster()

**Syntax:** `geo_cluster(<geo_field>, <precision>)`

**Arguments:**

*   `<geo_field>`: a geopoint field
*   `<precision>`: an integer between 1 and 12 defining the clustering precision (higher = smaller clusters)

**Clauses where it can be used:** `group_by` only

**Returned type:** geometry

Groups geo points into geographic clusters based on proximity. The precision parameter controls the size of the clusters:

*   Lower values (1-3): very large clusters (country/continent level)
*   Medium values (4-8): city/neighborhood level
*   Higher values (9-12): street/building level

> Examples of geo\_cluster():

```sql
select=count(*) & group_by=geo_cluster(location, 5)  -- Count records in each geographic cluster

select=avg(price), geo_cluster(location, 7) AS cluster & group_by=geo_cluster(location, 7)  -- Average price per cluster
```

# Versioning

The Explore API uses versioning to ensure backward compatibility and allow for API evolution.

## Deprecation warnings

When a feature or endpoint is deprecated, the API includes deprecation warnings in the response headers:

*   `X-ODS-API-Deprecation-Date`: The date when the feature was deprecated
*   `X-ODS-API-Sunset-Date`: The date when the feature will be removed

Clients should monitor these headers and update their integration before the sunset date.

# Catalog

The catalog endpoints provide access to the list of available datasets on a domain.

## getQuery catalog datasets

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets`

**Description:** Search and filter the catalog of datasets.

**Query Parameters:**

*   `select`: Select clause to choose returned fields
*   `where`: Where clause to filter datasets
*   `group_by`: Group by clause for aggregations
*   `order_by`: Order by clause to sort results
*   `limit`: Maximum number of results (default: 10, max: 100)
*   `offset`: Offset for pagination (default: 0)
*   `refine`: Facet refinement
*   `exclude`: Facet exclusion
*   `timezone`: Timezone for date/time operations

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets?where=modified>date'2023-01-01'&limit=20
```

## getList export formats

**Endpoint:** `GET /api/explore/v2.1/catalog/exports`

**Description:** Get the list of available export formats for the catalog.

**Example:**

```http
GET /api/explore/v2.1/catalog/exports
```

## getExport a catalog

**Endpoint:** `GET /api/explore/v2.1/catalog/exports/{format}`

**Description:** Export the catalog in the specified format.

**Path Parameters:**

*   `format`: Export format (csv, json, xlsx, etc.)

**Query Parameters:**

*   Same as getQuery catalog datasets

**Example:**

```http
GET /api/explore/v2.1/catalog/exports/json?where=publisher:"City of Paris"
```

## getExport a catalog in CSV

**Endpoint:** `GET /api/explore/v2.1/catalog/exports/csv`

**Description:** Export the catalog in CSV format.

**Additional Query Parameters:**

*   `delimiter`: Field delimiter (default: `;`)
*   `list_separator`: List value separator (default: `,`)
*   `quote_all`: Quote all fields (default: false)

**Example:**

```http
GET /api/explore/v2.1/catalog/exports/csv?delimiter=,&quote_all=true
```

## getExport a catalog in RDF/XML (DCAT)

**Endpoint:** `GET /api/explore/v2.1/catalog/exports/rdfxml`

**Description:** Export the catalog in RDF/XML format following the DCAT specification.

**Example:**

```http
GET /api/explore/v2.1/catalog/exports/rdfxml
```

## getList facet values

**Endpoint:** `GET /api/explore/v2.1/catalog/facets`

**Description:** Get available facet values for the catalog.

**Query Parameters:**

*   `facet`: Facet field name(s)
*   `refine`: Facet refinement
*   `exclude`: Facet exclusion

**Example:**

```http
GET /api/explore/v2.1/catalog/facets?facet=publisher&facet=modified
```

## getShow dataset information

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}`

**Description:** Get detailed information about a specific dataset.

**Path Parameters:**

*   `dataset_id`: Unique identifier of the dataset

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices
```

# Dataset

The dataset endpoints provide access to the records within a specific dataset.

## getQuery dataset records

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/records`

**Description:** Query and retrieve records from a dataset.

**Path Parameters:**

*   `dataset_id`: Unique identifier of the dataset

**Query Parameters:**

*   `select`: Select clause to choose returned fields
*   `where`: Where clause to filter records
*   `group_by`: Group by clause for aggregations
*   `order_by`: Order by clause to sort results
*   `limit`: Maximum number of results (default: 10, max: 100 for records, 20000 for groups)
*   `offset`: Offset for pagination (default: 0)
*   `refine`: Facet refinement
*   `exclude`: Facet exclusion
*   `timezone`: Timezone for date/time operations

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/records?where=year(date)=2020&limit=50
```

## getList export formats

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/exports`

**Description:** Get the list of available export formats for the dataset.

**Path Parameters:**

*   `dataset_id`: Unique identifier of the dataset

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/exports
```

## getExport a dataset

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/exports/{format}`

**Description:** Export dataset records in the specified format.

**Path Parameters:**

*   `dataset_id`: Unique identifier of the dataset
*   `format`: Export format (csv, json, xlsx, parquet, geojson, etc.)

**Query Parameters:**

*   Same as getQuery dataset records

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/exports/json?where=price>500
```

## getExport a dataset in CSV

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/exports/csv`

**Description:** Export dataset records in CSV format.

**Additional Query Parameters:**

*   `delimiter`: Field delimiter (default: `;`)
*   `list_separator`: List value separator (default: `,`)
*   `quote_all`: Quote all fields (default: false)
*   `bom`: Include Byte Order Mark (default: true in v2.1)

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/exports/csv?delimiter=,
```

## getExport a dataset in Parquet

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/exports/parquet`

**Description:** Export dataset records in Apache Parquet format for efficient data processing.

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/exports/parquet?where=year(date)>=2020
```

## getExport a dataset in GPX

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/exports/gpx`

**Description:** Export dataset records with geographic data in GPX format.

**Additional Query Parameters:**

*   `use_extension`: Use GPX extensions for attributes (default: true in v2.1)

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/bicycle-routes/exports/gpx
```

## getList dataset facets

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/facets`

**Description:** Get available facet values for a dataset.

**Path Parameters:**

*   `dataset_id`: Unique identifier of the dataset

**Query Parameters:**

*   `facet`: Facet field name(s)
*   `refine`: Facet refinement
*   `exclude`: Facet exclusion
*   `where`: Where clause to filter

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/facets?facet=year(date)
```

## getList dataset attachments

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/attachments`

**Description:** Get the list of files attached to a dataset.

**Path Parameters:**

*   `dataset_id`: Unique identifier of the dataset

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/attachments
```

## getRead a dataset record

**Endpoint:** `GET /api/explore/v2.1/catalog/datasets/{dataset_id}/records/{record_id}`

**Description:** Get detailed information about a specific record.

**Path Parameters:**

*   `dataset_id`: Unique identifier of the dataset
*   `record_id`: Unique identifier of the record

**Example:**

```http
GET /api/explore/v2.1/catalog/datasets/gold-prices/records/abc123
```

---

**Documentation Source:** [https://help.opendatasoft.com/apis/ods-explore-v2/](https://help.opendatasoft.com/apis/ods-explore-v2/)

**API Version:** v2.1

**Last Updated:** Retrieved 2025-11-16
